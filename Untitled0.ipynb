{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrtKcl/JpeASRwC0RhPCjw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackyySong/Pacman-practice/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = \"/content/drive/My Drive/car_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "data = df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywYTGVJ0qHlP",
        "outputId": "50d3cdbd-283a-4085-b452-164a43f96c54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLPjhO2kqpgl",
        "outputId": "dc27e431-9adc-40c2-f4db-b1baf02ba1be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User ID Gender  Age  AnnualSalary  Purchased\n",
            "0      385   Male   35         20000          0\n",
            "1      681   Male   40         43500          0\n",
            "2      353   Male   49         74000          0\n",
            "3      895   Male   40        107500          1\n",
            "4      661   Male   25         79000          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 从数据中删除 'User ID' 列 delete 'User ID'\n",
        "if \"User ID\" in df.columns:\n",
        "    df = df.drop(columns=[\"User ID\"])\n",
        "# 移除原有的连续年薪列  Remove the original continuous annual salary column\n",
        "if \"AnnualSalary\" in df.columns:\n",
        "    df['AnnualSalary_Category'] = pd.cut(df['AnnualSalary'], bins=[0, 50000, 100000, float('inf')], labels=['Low', 'Medium', 'High'])\n",
        "    df.drop(columns=[\"AnnualSalary\"], inplace=True)  # 移除原有的连续年薪列\n",
        "\n",
        "# 80% 训练，20% 临时数据  80% For train date and 20% for temp\n",
        "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# 将临时数据平均分成验证集和测试集  Divide the provisional data equally into validation sets and test sets\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training Data:\", len(train_data))\n",
        "print(\"Validation Data:\", len(valid_data))\n",
        "print(\"Test Data:\", len(test_data))\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 获取目标变量的类别和其对应的数量 Gets the category of the target variable and its corresponding quantity\n",
        "class_counts = train_data['Purchased'].value_counts()\n",
        "\n",
        "# 计算最大数量  Calculate maximum quantity\n",
        "max_count = class_counts.max()\n",
        "\n",
        "# 为每个类别计算权重  Calculate weights for each category\n",
        "class_weights = {cls: max_count/count for cls, count in class_counts.items()}\n",
        "\n",
        "# 使用加权决策树分类器  Using a weighted decision tree classifier\n",
        "clf = DecisionTreeClassifier(class_weight=class_weights)\n",
        "\n",
        "# 使用除\"Purchased\"列之外的所有列作为特征  Use all columns except the column \"Purchased\" as features\n",
        "X_train = pd.get_dummies(train_data.drop(columns=['Purchased']))\n",
        "y_train = train_data['Purchased']\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "def info_gain(data, split_attribute, target_name=\"Purchased\"):\n",
        "    total_entropy = entropy(data[target_name])\n",
        "\n",
        "    vals, counts = np.unique(data[split_attribute], return_counts=True)\n",
        "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n",
        "\n",
        "    # 打印整个数据集的熵  Print the entropy of the entire data set\n",
        "print(\"Total Entropy of Training Data:\", entropy(train_data[\"Purchased\"]))\n",
        "\n",
        "# 打印基于每个属性的信息增益（除目标属性\"Purchased\"外）  Prints information gain based on each property (except the target property \"Purchased\")\n",
        "for feature in train_data.columns[:-1]:\n",
        "    print(f\"Information Gain for '{feature}':\", info_gain(train_data, feature, \"Purchased\"))\n",
        "\n",
        "\n",
        "def decision_tree(data, original_data, features, target_name=\"Purchased\", parent_node_class=None, depth=0, max_depth=None):\n",
        "\n",
        "    if max_depth is not None and depth >= max_depth:\n",
        "        return np.unique(data[target_name])[np.argmax(np.unique(data[target_name], return_counts=True)[1])]\n",
        "\n",
        "    # 1. 如果数据中的目标属性只有一个值，则返回该值  If the target property in the data has only one value, it is returned\n",
        "    if len(np.unique(data[target_name])) == 1:\n",
        "        return np.unique(data[target_name])[0]\n",
        "\n",
        "    # 2. 如果数据集为空，返回原始数据中最常见的目标属性值 If the dataset is empty, return the value of the most common target attribute in the original data\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(original_data[target_name])[np.argmax(np.unique(original_data[target_name], return_counts=True)[1])]\n",
        "\n",
        "    # 3. 如果特征为空，返回父节点的类  If the feature is empty, return the class of the parent node\n",
        "    elif len(features) == 0:\n",
        "        return parent_node_class\n",
        "\n",
        "    # 4. 否则，我们选择一个特征来分割  Otherwise, we choose a feature to divide\n",
        "    else:\n",
        "        # 设置默认值为当前节点的最常见值  Set the default value to the most common value for the current node\n",
        "        parent_node_class = np.unique(data[target_name])[np.argmax(np.unique(data[target_name], return_counts=True)[1])]\n",
        "\n",
        "        # 选择信息增益最大的特征，确保“AnnualSalary”是首选  Select the feature with the greatest information gain and make sure \"AnnualSalary\" is preferred\n",
        "        item_values = [info_gain(data, feature, target_name) + (0.01 if feature == \"AnnualSalary\" else 0) for feature in features]\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "\n",
        "        # 为该特征创建树结构  Create a tree structure for the feature\n",
        "        tree = {best_feature: {}}\n",
        "\n",
        "        # 除了最佳的特征外，剔除所有其他特征  Eliminate all but the best features\n",
        "        features = [i for i in features if i != best_feature]\n",
        "\n",
        "        # 为每一个特征值增加一个子树  Add a subtree for each eigenvalue\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            subtree = decision_tree(sub_data, data, features, target_name, parent_node_class, depth+1, max_depth)\n",
        "            tree[best_feature][value] = subtree\n",
        "\n",
        "        return tree\n",
        "\n",
        "tree = decision_tree(train_data, train_data, train_data.columns[:-1])\n",
        "\n",
        "print(tree)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AByPvkuD3P75",
        "outputId": "f2f8cdfc-e3ce-4a76-ea04-399c5d327767"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: 800\n",
            "Validation Data: 100\n",
            "Test Data: 100\n",
            "Total Entropy of Training Data: 0.9663939498942491\n",
            "Information Gain for 'Gender': 0.0008015137451309329\n",
            "Information Gain for 'Age': 0.37569631386786306\n",
            "{'Age': {18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 29: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 30: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 31: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 32: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 33: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 34: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 35: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 36: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 37: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 38: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 39: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 40: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 41: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 42: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 43: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 44: {'Gender': {'Female': 0.0, 'Male': 0.0}}, 45: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 46: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 47: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 48: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 49: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 50: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 51: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 52: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 53: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 54: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 55: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 56: 1.0, 57: 1.0, 58: 1.0, 59: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 60: {'Gender': {'Female': 1.0, 'Male': 1.0}}, 61: 1.0, 62: {'Gender': {'Female': 1.0}}, 63: 1.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rnw6x2N6ncsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict(query, tree, default=0):\n",
        "    \"\"\"\n",
        "    使用决策树预测单个查询  Use decision trees to predict individual queries\n",
        "\n",
        "    :param query: 要预测的数据点  Data points to predict\n",
        "    :param tree: 决策树模型  Decision tree model\n",
        "    :param default: 默认返回值（如果找不到匹配项）  Default return value (if no match is found)\n",
        "    :return: 预测的类标签  Predicted class tags\n",
        "    \"\"\"\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]]\n",
        "            except:\n",
        "                return default\n",
        "            if isinstance(result, dict):\n",
        "                return predict(query, result)\n",
        "            else:\n",
        "                return result\n",
        "def evaluate(tree, data):\n",
        "    \"\"\"\n",
        "    评估模型在给定数据上的准确度  Evaluate the accuracy of the model on the given data\n",
        "\n",
        "    :param tree: 决策树模型  Decision tree model\n",
        "    :param data: 要评估的数据集  The data set to be evaluated\n",
        "    :return: 准确度 (0-1之间)  Accuracy (between 0-1)\n",
        "    \"\"\"\n",
        "    queries = data.iloc[:, :-1].to_dict(orient=\"records\")\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"])\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i, \"predicted\"] = predict(queries[i], tree, 1.0)  # 1.0 作为默认值\n",
        "\n",
        "    # 重置索引以进行比较  Reset the index for comparison\n",
        "    predicted.reset_index(drop=True, inplace=True)\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    accuracy = sum(predicted[\"predicted\"] == data[\"Purchased\"]) / len(data) * 100\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate(tree, valid_data)\n",
        "print(f\"Accuracy on Validation Data: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        " # 加权损失函数预测  Weighted loss function prediction\n",
        "def preprocess_data(data):\n",
        "    # 进行One-hot编码\n",
        "    data_transformed = pd.get_dummies(data)\n",
        "    return data_transformed\n",
        "\n",
        "# 预处理train_data  preconditioning\n",
        "X_train = preprocess_data(train_data.drop(columns=['Purchased']))\n",
        "y_train = train_data['Purchased']\n",
        "\n",
        "# 预处理valid_data  preconditioning\n",
        "X_valid = preprocess_data(valid_data.drop(columns=['Purchased']))\n",
        "y_valid = valid_data['Purchased']\n",
        "\n",
        "# 预处理test_data  preconditioning\n",
        "X_test = preprocess_data(test_data.drop(columns=['Purchased']))\n",
        "y_test = test_data['Purchased']\n",
        "\n",
        "# 训练模型  Training model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 对valid_data进行评估  Evaluate valid_data\n",
        "predictions = clf.predict(X_valid)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 计算准确度  Accuracy of calculation\n",
        "accuracy = accuracy_score(y_valid, predictions)\n",
        "\n",
        "print(f\"Accuracy on Validation Data: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFqdWt674jV",
        "outputId": "42aa1e6b-50c4-40df-9499-e4b45549b509"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation Data: 81.00%\n",
            "Accuracy on Validation Data: 75.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 首先定义gini_impurity函数：  First define the gini_impurity function:\n",
        "def gini_impurity(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    gini = 1 - sum([(counts[i]/sum(counts))**2 for i in range(len(elements))])\n",
        "    return gini\n",
        "\n",
        "# 检查数据不平衡  Check data imbalance\n",
        "purchase_counts = train_data['Purchased'].value_counts()\n",
        "print(\"\\nPurchased Counts:\")\n",
        "print(purchase_counts)\n",
        "\n",
        "# 计算每个特征的信息增益  Calculate the information gain for each feature\n",
        "print(\"\\nInformation Gains:\")\n",
        "for feature in train_data.columns[:-1]:  # 不包括目标列 \"Purchased\"\n",
        "    print(f\"Information Gain for '{feature}':\", info_gain(train_data, feature, \"Purchased\"))\n",
        "\n",
        "# 计算每个特征的基尼增益  Calculate the Gini gain for each feature\n",
        "print(\"\\nGini Gains:\")\n",
        "for feature in train_data.columns[:-1]:  # 不包括目标列 \"Purchased\"\n",
        "    total_gini = gini_impurity(train_data[\"Purchased\"])\n",
        "    vals, counts = np.unique(train_data[feature], return_counts=True)\n",
        "    weighted_gini = sum([(counts[i] / sum(counts)) * gini_impurity(train_data.where(train_data[feature] == vals[i]).dropna()[\"Purchased\"]) for i in range(len(vals))])\n",
        "    gini_gain = total_gini - weighted_gini\n",
        "    print(f\"Gini Gain for '{feature}':\", gini_gain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBGtqJ2FAnxO",
        "outputId": "d8730ddb-482b-4b6f-c9e4-aa3f27fce3f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Purchased Counts:\n",
            "0    486\n",
            "1    314\n",
            "Name: Purchased, dtype: int64\n",
            "\n",
            "Information Gains:\n",
            "Information Gain for 'Gender': 0.0008015137451309329\n",
            "Information Gain for 'Age': 0.37569631386786306\n",
            "\n",
            "Gini Gains:\n",
            "Gini Gain for 'Gender': 0.0005295865560310808\n",
            "Gini Gain for 'Age': 0.20852368583961378\n"
          ]
        }
      ]
    }
  ]
}